// Exercise 20.5: Transfer Learning for Domain Adaptation in Rust
// Using Candle with Phi-2 model
//
// This implementation demonstrates:
// 1. Loading a pre-trained LLM (Phi-2)
// 2. Fine-tuning on domain-specific data (Rust programming)
// 3. Multiple transfer learning strategies (full fine-tuning vs layer freezing)
// 4. Performance evaluation before and after adaptation

use anyhow::Result;
use candle_core::{DType, Device, IndexOp, Tensor};
use candle_nn::{loss, linear_no_bias, Optimizer, VarBuilder, VarMap};
use candle_transformers::models::qwen2::{Config as Qwen2Config, Model as Qwen2Model};
use candle_nn::Linear;
use hf_hub::api::sync::Api;
use tokenizers::Tokenizer;
use std::collections::HashMap;

// ============================================================================
// MODEL WRAPPER
// ============================================================================

struct Qwen2ModelWithHead {
    model: Qwen2Model,
    lm_head: Linear,
}

impl Qwen2ModelWithHead {
    fn forward_all_tokens(&mut self, input_ids: &Tensor) -> Result<Tensor> {
        // Clear KV cache to start fresh
        self.model.clear_kv_cache();

        // Process tokens autoregressively (one at a time)
        // This is what Qwen2 expects for KV cache to work correctly
        let seq_len = input_ids.dim(1)?;
        let mut all_logits = Vec::new();

        for pos in 0..seq_len {
            // Get single token at position pos
            let token = input_ids.i((.., pos..pos+1))?;

            // Forward pass with correct position
            let hidden_states = self.model.forward(&token, pos, None)?;
            let logits = hidden_states.apply(&self.lm_head)?;

            all_logits.push(logits);
        }

        // Concatenate all logits along sequence dimension
        Ok(Tensor::cat(&all_logits, 1)?)
    }

    fn clear_kv_cache(&mut self) {
        self.model.clear_kv_cache()
    }
}

// ============================================================================
// EVALUATION & GENERATION
// ============================================================================

/// Calculate perplexity on a test set
fn calculate_perplexity(
    model: &mut Qwen2ModelWithHead,
    test_inputs: &Tensor,
    test_labels: &Tensor,
) -> Result<f32> {
    // Get logits for all tokens
    let logits = model.forward_all_tokens(test_inputs)?;

    let (b_size, t_size, v_size) = logits.dims3()?;
    let logits_flat = logits.reshape((b_size * t_size, v_size))?;
    let labels_flat = test_labels.reshape((b_size * t_size,))?;

    let loss_val = loss::cross_entropy(&logits_flat, &labels_flat)?;
    let perplexity = loss_val.to_scalar::<f32>()?.exp();

    Ok(perplexity)
}

/// Greedy sampling for text generation
fn generate_text(
    model: &mut Qwen2ModelWithHead,
    tokenizer: &Tokenizer,
    device: &Device,
    prompt: &str,
    max_tokens: usize,
) -> Result<String> {
    let tokens = tokenizer
        .encode(prompt, true)
        .map_err(anyhow::Error::msg)?
        .get_ids()
        .to_vec();
    let mut all_tokens = tokens.clone();

    for _ in 0..max_tokens {
        model.clear_kv_cache();  // Clear cache before each generation step
        let tokens_tensor = Tensor::new(all_tokens.as_slice(), device)?.unsqueeze(0)?;
        // Get logits for all tokens
        let logits = model.forward_all_tokens(&tokens_tensor)?;

        // Get last token logits from [batch, seq_len, vocab_size]
        let seq_len = logits.dim(1)?;
        let last_logits = logits.i((0, seq_len - 1))?; // [vocab_size]

        let next_token_id = last_logits.argmax(0)?.to_scalar::<u32>()?;
        all_tokens.push(next_token_id);
    }

    tokenizer.decode(&all_tokens, false).map_err(anyhow::Error::msg)
}

/// Evaluate model on multiple prompts
fn evaluate_on_prompts(
    model: &mut Qwen2ModelWithHead,
    tokenizer: &Tokenizer,
    device: &Device,
    prompts: &[&str],
) {
    println!("\n=== Model Evaluation ===");
    for prompt in prompts {
        println!("\nPrompt: \"{}\"", prompt);
        match generate_text(model, tokenizer, device, prompt, 30) {  // Qwen2-0.5B is fast enough
            Ok(text) => println!("Generated: \"{}\"", text),
            Err(e) => println!("Error: {}", e),
        }
    }
}

// ============================================================================
// DATASET PREPARATION
// ============================================================================

fn prepare_domain_dataset() -> String {
    // Domain-specific dataset: Rust programming concepts
    r#"
    In Rust, ownership is a set of rules that govern memory management.
    The Vec<T> type is a growable array stored on the heap.
    Result<T, E> is used for error handling in Rust programs.
    The match expression enables pattern matching on enums.
    Traits define shared behavior in an abstract way.
    The borrow checker ensures memory safety at compile time.
    Cargo is the Rust package manager and build system.
    impl blocks define methods on structs and enums.
    The Option<T> enum handles the presence or absence of values.
    Lifetimes ensure references are valid for their entire scope.
    The String type is a growable UTF-8 encoded string.
    Closures are anonymous functions that can capture their environment.
    The derive attribute automatically implements traits.
    Async/await syntax enables asynchronous programming.
    The candle crate provides tensor operations for machine learning.
    Error propagation uses the ? operator for Result types.
    Generic types enable code reuse with type parameters.
    The println! macro prints formatted text to stdout.
    References borrow values without taking ownership.
    Mutable references allow modification of borrowed data.
    "#.to_string()
}

fn prepare_test_dataset() -> String {
    // Separate test set for evaluation
    r#"
    Rust's ownership system prevents data races at compile time.
    The HashMap<K, V> type stores key-value pairs.
    Iterators provide a way to process sequences of elements.
    The std::fs module provides file system operations.
    Smart pointers like Box<T> enable heap allocation.
    "#.to_string()
}

fn tokenize_dataset(
    text: &str,
    tokenizer: &Tokenizer,
    device: &Device,
    block_size: usize,
) -> Result<(Tensor, Tensor)> {
    let tokens = tokenizer
        .encode(text, true)
        .map_err(anyhow::Error::msg)?
        .get_ids()
        .to_vec();

    let actual_block_size = block_size.min(tokens.len() - 1);
    let inputs_vec: Vec<u32> = tokens[..actual_block_size].to_vec();
    let labels_vec: Vec<u32> = tokens[1..=actual_block_size].to_vec();

    let inputs = Tensor::new(inputs_vec.as_slice(), device)?.unsqueeze(0)?;
    let labels = Tensor::new(labels_vec.as_slice(), device)?.unsqueeze(0)?;

    Ok((inputs, labels))
}

// ============================================================================
// TRANSFER LEARNING STRATEGIES
// ============================================================================

enum TransferStrategy {
    FullFineTune,           // Train all parameters
    FreezeLowerLayers,      // Freeze first 50% of layers
    FreezeEmbeddings,       // Only freeze embedding layers
    AdapterLayers,          // Freeze all, train only specific layers (simulated)
}

fn apply_transfer_strategy(
    varmap: &VarMap,
    strategy: &TransferStrategy,
    n_layers: usize,
) -> Vec<candle_core::Var> {
    // Get the data as a HashMap<String, Var> to access parameter names
    let var_data = varmap.data();
    let var_data = var_data.lock().unwrap();

    match strategy {
        TransferStrategy::FullFineTune => {
            println!("Strategy: Full Fine-Tuning (all {} parameters)", var_data.len());
            var_data.values().cloned().collect()
        }
        TransferStrategy::FreezeLowerLayers => {
            let freeze_count = n_layers / 2;
            println!("Strategy: Freeze Lower Layers (freeze first {})", freeze_count);

            let trainable: Vec<_> = var_data.iter()
                .filter(|(name, _var)| {
                    // For Qwen2, layers are named like "model.layers.0", "model.layers.1", etc.
                    // Train upper half of layers
                    let mut should_train = true;
                    for i in 0..freeze_count {
                        if name.contains(&format!("layers.{}", i)) {
                            should_train = false;
                            break;
                        }
                    }
                    should_train
                })
                .map(|(_name, var)| var.clone())
                .collect();

            println!("  Trainable parameters: {}/{}", trainable.len(), var_data.len());
            trainable
        }
        TransferStrategy::FreezeEmbeddings => {
            println!("Strategy: Freeze Embeddings Only");

            let trainable: Vec<_> = var_data.iter()
                .filter(|(name, _var)| {
                    // Freeze embedding layers (embed_tokens in Qwen2)
                    !name.contains("embed_tokens")
                })
                .map(|(_name, var)| var.clone())
                .collect();

            println!("  Trainable parameters: {}/{}", trainable.len(), var_data.len());
            trainable
        }
        TransferStrategy::AdapterLayers => {
            println!("Strategy: Adapter Layers (train last 2 layers + head)");

            let trainable: Vec<_> = var_data.iter()
                .filter(|(name, _var)| {
                    // Only train:
                    // - Last 2 transformer layers (layers.22 and layers.23 for Qwen2-0.5B with 24 layers)
                    // - Final layer norm (norm)
                    // - LM head
                    name.contains("lm_head") ||
                    name.contains(&format!("layers.{}", n_layers - 1)) ||
                    name.contains(&format!("layers.{}", n_layers - 2)) ||
                    (name.contains("norm") && !name.contains("layers."))
                })
                .map(|(_name, var)| var.clone())
                .collect();

            println!("  Trainable parameters: {}/{}", trainable.len(), var_data.len());
            trainable
        }
    }
}

// ============================================================================
// TRAINING LOOP
// ============================================================================

fn train_model(
    model: &mut Qwen2ModelWithHead,
    train_data: &(Tensor, Tensor),
    test_data: &(Tensor, Tensor),
    trainable_params: Vec<candle_core::Var>,
    learning_rate: f64,
    epochs: usize,
) -> Result<Vec<f32>> {
    let mut optimizer = candle_nn::AdamW::new_lr(trainable_params.clone(), learning_rate)?;
    let mut losses = Vec::new();

    println!("\nTraining with {} trainable parameters", trainable_params.len());
    println!("Epochs: {}, Learning Rate: {}", epochs, learning_rate);
    println!("\n{:<8} {:<12} {:<12}", "Epoch", "Train Loss", "Perplexity");
    println!("{}", "-".repeat(35));

    for epoch in 0..epochs {
        // Clear KV cache before each epoch
        model.clear_kv_cache();

        // Forward pass - get logits for all tokens
        let logits = model.forward_all_tokens(&train_data.0)?;

        let (b_size, t_size, v_size) = logits.dims3()?;
        let logits_flat = logits.reshape((b_size * t_size, v_size))?;
        let labels_flat = train_data.1.reshape((b_size * t_size,))?;

        // Calculate loss
        let loss_val = loss::cross_entropy(&logits_flat, &labels_flat)?;
        let loss_scalar = loss_val.to_scalar::<f32>()?;
        losses.push(loss_scalar);

        // Backward pass
        optimizer.backward_step(&loss_val)?;

        // Evaluate every 5 epochs to track progress
        if epoch % 5 == 0 || epoch == epochs - 1 {
            let perplexity = calculate_perplexity(model, &test_data.0, &test_data.1)?;
            println!("{:<8} {:<12.6} {:<12.2}", epoch, loss_scalar, perplexity);
        }
    }

    Ok(losses)
}

// ============================================================================
// MAIN EXPERIMENT
// ============================================================================

fn main() -> Result<()> {
    println!("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘  Exercise 20.5: Transfer Learning for Domain Adaptation   â•‘");
    println!("â•‘  Qwen2-0.5B - Adaptive GPU/CPU with Memory Optimization   â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // ========================================================================
    // Setup - Hybrid GPU/CPU with automatic fallback
    // ========================================================================
    println!("ğŸ”§ Setting up environment...");

    // Try to use CUDA, fall back to CPU if unavailable
    let device = match Device::new_cuda(0) {
        Ok(cuda_device) => {
            println!("   âœ“ CUDA GPU detected: Using GPU");
            println!("   âœ“ Memory optimization: Small batch size + Adapter strategy");
            cuda_device
        }
        Err(_) => {
            println!("   âš  CUDA not available: Falling back to CPU");
            Device::Cpu
        }
    };

    // Use FP32 (Qwen2 has some ops that don't support FP16 well)
    // Memory savings come from: smaller batch + training only adapters
    let dtype = DType::F32;
    println!("   Device: {:?}, DType: {:?}", device, dtype);

    let mut varmap = VarMap::new();
    let vb = VarBuilder::from_varmap(&varmap, dtype, &device);

    // ========================================================================
    // Load Pre-trained Model
    // ========================================================================
    println!("\nğŸ“¦ Loading pre-trained Qwen2-0.5B model...");

    // Set HuggingFace endpoint explicitly to avoid URL parsing issues
    unsafe {
        std::env::set_var("HF_ENDPOINT", "https://huggingface.co");
    }

    let api = Api::new()?;
    let repo = api.model("Qwen/Qwen2-0.5B".to_string());

    let tokenizer_file = repo.get("tokenizer.json")?;
    let tokenizer = Tokenizer::from_file(tokenizer_file).map_err(anyhow::Error::msg)?;
    println!("   âœ“ Tokenizer loaded");

    let config_file = repo.get("config.json")?;
    let config: Qwen2Config = serde_json::from_str(&std::fs::read_to_string(config_file)?)?;
    println!("   âœ“ Config loaded");

    // Qwen2-0.5B uses a single weights file (much smaller!)
    let weights_file = repo.get("model.safetensors")?;
    varmap.load(&weights_file)?;
    println!("   âœ“ Weights loaded");

    let base_model = Qwen2Model::new(&config, vb.clone())?;
    let lm_head = linear_no_bias(config.hidden_size, config.vocab_size, vb.pp("lm_head"))?;
    let mut model = Qwen2ModelWithHead {
        model: base_model,
        lm_head,
    };
    println!("   âœ“ Model initialized");

    // ========================================================================
    // Prepare Datasets
    // ========================================================================
    println!("\nğŸ“š Preparing domain-specific datasets...");
    let train_text = prepare_domain_dataset();
    let test_text = prepare_test_dataset();

    // Create datasets with small sequences optimized for memory
    let block_size = 4;  // Reduced from 8 for better GPU memory efficiency
    let train_data = tokenize_dataset(&train_text, &tokenizer, &device, block_size)?;
    let test_data = tokenize_dataset(&test_text, &tokenizer, &device, block_size)?;
    println!("   Train shape: {:?}", train_data.0.shape());
    println!("   Test shape: {:?}", test_data.0.shape());

    // ========================================================================
    // Baseline Evaluation (Before Fine-tuning)
    // ========================================================================
    println!("\nğŸ“Š Baseline Evaluation (Pre-trained model)");
    let baseline_perplexity = calculate_perplexity(&mut model, &test_data.0, &test_data.1)?;
    println!("   Initial Perplexity: {:.2}", baseline_perplexity);

    let eval_prompts = vec![
        "In Rust, Vec<T> is",
        "The Result type in Rust",
        "Ownership in Rust means",
    ];
    evaluate_on_prompts(&mut model, &tokenizer, &device, &eval_prompts);

    // Save weights file path for reloading
    let weights_path = weights_file.clone();

    // ========================================================================
    // Experiment with Different Transfer Learning Strategies
    // ========================================================================
    println!("\n\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘           TRANSFER LEARNING STRATEGY COMPARISON           â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

    // Order strategies from least to most memory intensive
    // This allows GPU to handle lighter strategies even if heavier ones fail
    let strategies = vec![
        ("Adapter Layers", TransferStrategy::AdapterLayers),           // Lightest: 6% params
        ("Freeze Lower Layers", TransferStrategy::FreezeLowerLayers),  // Medium: 64% params
        ("Freeze Embeddings", TransferStrategy::FreezeEmbeddings),     // Medium-Heavy: 72% params
        ("Full Fine-Tuning", TransferStrategy::FullFineTune),          // Heaviest: 100% params
    ];

    let mut results: HashMap<String, (Vec<f32>, f32)> = HashMap::new();

    for (name, strategy) in strategies {
        println!("\n{}", "=".repeat(60));
        println!(" {} ", name);
        println!("{}", "=".repeat(60));

        // Create fresh VarMap and model for each strategy
        let mut strategy_varmap = VarMap::new();
        strategy_varmap.load(&weights_path)?;
        let strategy_vb = VarBuilder::from_varmap(&strategy_varmap, DType::F32, &device);
        let fresh_base_model = Qwen2Model::new(&config, strategy_vb.clone())?;
        let fresh_lm_head = linear_no_bias(config.hidden_size, config.vocab_size, strategy_vb.pp("lm_head"))?;
        let mut fresh_model = Qwen2ModelWithHead {
            model: fresh_base_model,
            lm_head: fresh_lm_head,
        };

        // Clear KV cache before training
        fresh_model.clear_kv_cache();

        // Apply strategy
        let trainable_params = apply_transfer_strategy(&strategy_varmap, &strategy, config.num_hidden_layers);

        // Try training on current device, fall back to CPU on OOM
        let (losses, final_perplexity, used_device) = match train_model(
            &mut fresh_model,
            &train_data,
            &test_data,
            trainable_params.clone(),
            1e-4,
            20,
        ) {
            Ok(losses) => {
                // Training succeeded on current device
                let perp = calculate_perplexity(&mut fresh_model, &test_data.0, &test_data.1)?;
                (losses, perp, format!("{:?}", device))
            }
            Err(e) if e.to_string().contains("out of memory") => {
                // OOM on GPU - try CPU fallback
                println!("\nâš  GPU Out of Memory - Falling back to CPU for this strategy");

                // Reload model on CPU
                let cpu_device = Device::Cpu;
                let mut cpu_varmap = VarMap::new();
                cpu_varmap.load(&weights_path)?;
                let cpu_vb = VarBuilder::from_varmap(&cpu_varmap, DType::F32, &cpu_device);
                let cpu_base_model = Qwen2Model::new(&config, cpu_vb.clone())?;
                let cpu_lm_head = linear_no_bias(config.hidden_size, config.vocab_size, cpu_vb.pp("lm_head"))?;
                let mut cpu_model = Qwen2ModelWithHead {
                    model: cpu_base_model,
                    lm_head: cpu_lm_head,
                };

                // Prepare data on CPU
                let cpu_train_data = tokenize_dataset(&train_text, &tokenizer, &cpu_device, block_size)?;
                let cpu_test_data = tokenize_dataset(&test_text, &tokenizer, &cpu_device, block_size)?;

                // Apply strategy on CPU model
                let cpu_trainable_params = apply_transfer_strategy(&cpu_varmap, &strategy, config.num_hidden_layers);

                // Train on CPU
                let cpu_losses = train_model(
                    &mut cpu_model,
                    &cpu_train_data,
                    &cpu_test_data,
                    cpu_trainable_params,
                    1e-4,
                    20,
                )?;

                // Evaluate on CPU
                let cpu_perp = calculate_perplexity(&mut cpu_model, &cpu_test_data.0, &cpu_test_data.1)?;
                fresh_model = cpu_model;  // Update model reference for generation
                (cpu_losses, cpu_perp, "Cpu (fallback)".to_string())
            }
            Err(e) => return Err(e),  // Other errors are fatal
        };

        println!("\nâœ“ Final Perplexity: {:.2} (Device: {})", final_perplexity, used_device);

        results.insert(name.to_string(), (losses, final_perplexity));

        // Generate samples after training
        let gen_device = if used_device.contains("Cpu") { Device::Cpu } else { device.clone() };
        evaluate_on_prompts(&mut fresh_model, &tokenizer, &gen_device, &eval_prompts);
    }

    // ========================================================================
    // Final Results Summary
    // ========================================================================
    println!("\n\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘                    RESULTS SUMMARY                         â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("\nBaseline (Pre-trained): {:.2} perplexity\n", baseline_perplexity);
    println!("{:<25} {:<15} {:<15}", "Strategy", "Final Perplexity", "Improvement");
    println!("{}", "-".repeat(60));

    for (strategy, (_, perplexity)) in results.iter() {
        let improvement = ((baseline_perplexity - perplexity) / baseline_perplexity) * 100.0;
        println!("{:<25} {:<15.2} {:<15.1}%", strategy, perplexity, improvement);
    }

    println!("\n\nğŸ“ Key Findings:");
    println!("   â€¢ Full fine-tuning typically achieves lowest perplexity");
    println!("   â€¢ Freezing strategies reduce training time & overfitting risk");
    println!("   â€¢ Adapter layers offer good trade-off for limited data");
    println!("   â€¢ Domain adaptation successful with minimal training data\n");

    Ok(())
}